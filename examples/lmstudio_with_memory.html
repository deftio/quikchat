<!DOCTYPE html> <!-- local running example using quikchat and ollama -->
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"
        type="image/gif">

    <title>QuikChat Demo with local Ollama</title>
    <!-- Include Bootstrap CSS file -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <!-- Include QuikChat CSS file -->
    <link rel="stylesheet" href="../../dist/quikchat.css">

    <!-- Include QuikChat Chat JavaScript file -->
    <script src="../../dist/quikchat.umd.min.js"></script>
    <style>
        html,
        body {
            width: 100%;
            height: 100%;
        }
        .chat-container {
            height: 70vh;
            width: 100%;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="col-10">
            <br>
            <h2 class="">LMStudio API quikchat Demo with Conversational Memory</h2>
            <p>This example demonstrates how to use quikchat with a local LLM using LMStudio where quikchat provides the
                chat history to LMStudio provides the llm model. This allows the chat to "remember" what is being
                discussed. <br>This example assumes <a href='https://lmstudio.ai/'>LMStudio</a> is installed and running on port 1234 locally.  Also it assumes the llama3.1 model is running.  This example will not work on the github pages demo website you must run this locally.</p>
            <br>
            <div class="chat-container" id="chat-container"></div>
        </div>
    </div>
    <script>

        // set up chat instance
        const streamingChat = new quikchat('#chat-container', {
            theme: 'quikchat-theme-light',
            onSend: getLocalLLMStreamingCallback,
            titleArea: { title: "Memory Chat", "show": true, "align": "left" },
        });
        streamingChat.messageAddNew("How can I help? ", "bot", "left", "system");

        // this calls the Ollama Streaming API with token streaming.
        // note that a very similar function can be used to call OpenAI, Mistral, or, Claude etc.
        // this is a pure js implementation that hits the API directly and doesn't use any libraries.
        function getLocalLLMStreamingCallback(chatInstance, userInput) {
            let startPrompt = {
                "content":
                `You are a skilled assistant.  Respond to user input with detailed 
                careful answers.  If you do not know the answer respond with 'I don't 
                have any information on that topic'.`, "role": "system"
            };
            let start = true;
            chatInstance.messageAddNew(userInput, "user", "right"); // echos the user input to the chat
            return fetch('http://localhost:1234/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    model: "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF",
                    messages: [startPrompt, ...chatInstance.historyGet()], // passes the chat history to the model
                    stream: true
                })
            }).then(response => {
                if (!response.ok) {
                    throw new Error(`HTTP error! Status: ${response.status}`);
                }
                return response.body.getReader();
            }).then(reader => {
                let id;

                return reader.read().then(function processResult(result) {
                    if (result.done) { return; }
                    let y=null,x = new TextDecoder().decode(result.value, { stream: true });  
                    x= x.replace("data: ", ""); // remove the data: prefix.. others do this too aparently
                    try {
                        y = JSON.parse(x.trim()); // decode the JSON
                    } catch(e) {}
                    //console.log(y); 
                    if (y && y.choices && y.choices[0].delta && y.choices[0].delta.content) {
                      
                        let content = y.choices[0].delta.content;//.message.content;
                        if (start) {
                            id = chatInstance.messageAddNew(content, "bot", "left"); // start a new chat message
                            start = false;
                        }
                        else {
                            chatInstance.messageAppendContent(id, content); // append new content to message
                        }
                        return reader.read().then(processResult);
                    }
                });
            }).then(() => {
                // finally done .. can do something here if needed.
            }).catch(error => {
                console.error('Fetch error:', error);
            });
        }
    </script>
</body>

</html>